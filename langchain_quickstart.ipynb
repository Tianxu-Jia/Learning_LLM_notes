{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "Two types of language models:\n",
    "\n",
    "-   LLMs: string input, string output, without reference to history, stateless model.\n",
    "-   ChatModels: list of  `ChatMessage` as input, output one  `ChatMessage`, reference to history, dynamic-history state memorised model\n",
    "\n",
    "A `ChatMessage` has two required components:\n",
    "\n",
    "-   `content`: The Content of the message.\n",
    "-   `role`: The role of the entity from which the `ChatMessage` is coming from. Who speak?\n",
    "\n",
    "LangChain provides several objects to easily distinguish between different roles:\n",
    "\n",
    "-   `HumanMessage`:  A `ChatMessage` coming from a human/user.\n",
    "-   `AIMessage`:  A `ChatMessage` coming from an AI/assistant.\n",
    "-   `SystemMessage`:  A `ChatMessage` coming from the system.  For example, xxx\n",
    "-   `FunctionMessage`:  A `ChatMessage` coming from a function call.\n",
    "\n",
    "If none of those roles sound right, there is also a `ChatMessage` class where you can specify the role manually. \n",
    "\n",
    "Two interface for both models:\n",
    "\n",
    "-   `predict`:  one String in, one string out\n",
    "-   `predict_messages`:  List of message in, one message out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/txjia/anaconda3/envs/llm311/lib/python3.11/site-packages/langchain/llms/openai.py:202: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/txjia/anaconda3/envs/llm311/lib/python3.11/site-packages/langchain/llms/openai.py:790: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "llm.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOpenAI()\n",
    "chat_model.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about interface `predict_messages`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: \"Vibrant Socks\"', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='SockSplash', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.generate([\"Hello, how are you?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=\"Hello! I'm an AI language model, so I don't have feelings, but I'm here to help. How can I assist you today?\", generation_info=None)]], llm_output={'token_usage': <OpenAIObject at 0x7f309ca9c890> JSON: {\n",
       "  \"prompt_tokens\": 13,\n",
       "  \"completion_tokens\": 30,\n",
       "  \"total_tokens\": 43\n",
       "}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('529e3457-51db-4c86-90be-c21d89934b6f'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant, so I don't have feelings, but I'm here to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "human_message = [HumanMessage(content=\"Translate this sentence from English to Chinese: I love programming.\")]\n",
    "chat_resp = chat_model(human_message)\n",
    "chat_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='我喜欢编程。 (Wǒ xǐhuān biānchéng.)', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='我喜欢编程。 (Wǒ xǐhuān biānchéng.)', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 19, 'completion_tokens': 24, 'total_tokens': 43}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('395db8c5-291a-4d0c-83cc-e9bb8ad40de2'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_resp2 = chat_model.generate([human_message])\n",
    "chat_resp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "Two models: \n",
    "\n",
    "        from langchain.llms import OpenAI\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "Three interface:\n",
    "\n",
    "        predict, predict_messages, (), generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
